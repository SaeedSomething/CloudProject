Phase 4 – HA proxy, Nginx and Auto scalable (2 point)
Step 1 – To ensure that the services can handle high loads and to prevent service failures, implement
automatic scaling for your microservices. This means that the services should automatically scale up
when they reach a specified high load threshold and scale down when the load decreases (No need to
scale the database).
Step 2 – For load distribution among the scaled services and to ensure access to the services, utilize an
internal NGINX instance with a custom routing configuration. This will help manage incoming
requests and distribute them evenly across the available instances of the services.
Step 3 – To enhance system resilience against increased load or potential failures, increase the number
of master nodes to two. Use external HAProxy to manage load balancing and synchronization
between the master nodes effectively.
In this phase, the scaling thresholds and custom routing configurations are your responsibility, and
you have the discretion to determine the parameters based on your project's needs.

Deadline
• 23rd Khordad
Delivery  
• Upload your yml files on VU
• Upload a short video (max 10 min) and show project works  
▪ Test and show project URLs works
▪ Show all pods, services, deployments, nodes, ... by details
▪ Explain Nginx, HA Proxy and auto scaling configurations

You are free to use ChatGPT or any other AI, but you are expected to learn.
